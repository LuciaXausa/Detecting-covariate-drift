{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to apply metrics on production data. We are considering two datasets: one in a 'normal' day and the second one in a 'drifted' day, i.e. a day when drifted occoured in producction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\xausa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xausa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('Utils'))\n",
    "sys.path.append(os.path.abspath('data'))\n",
    "sys.path.append(os.path.abspath('thresholds_and_results'))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils_driftSimulating import  create_black_folder\n",
    "from utils_resNet import init_resnet, df_from_folder\n",
    "from utils_dimRedDef import init_scaler, scale_dataset, initialize_DimReduction\n",
    "from utils_generateTests import reduced_on_drift_kdim, test_on_reduced_kdim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "seed_split = 1\n",
    "seed_drift = 10\n",
    "seed_metrics = 100\n",
    "info_dataset = [seed_split, seed_drift, seed_metrics]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of dimensions for dimensionality reduction\n",
    "k = 6\n",
    "# csv file where saving results\n",
    "resultFile = 'thresholds_and_results/6dim/prodResults_6d.csv'\n",
    "\n",
    "resultFile = 'thresholds_and_results/6dim/prod6d_prova.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories for original production data\n",
    "normal_path = 'data/original_data/normal/'\n",
    "drifted_path = 'data/original_data/drifted/'\n",
    "\n",
    "# directories where saving production black images\n",
    "normal_b_path = 'data/synthetic_data/normal_black/'\n",
    "drifted_b_path = 'data/synthetic_data/drifted_black/'\n",
    "\n",
    "# # apply black filter, to do just once because we apply black filter to production images\n",
    "# create_black_folder(normal_path, normal_b_path)\n",
    "# create_black_folder(drifted_path, drifted_b_path)\n",
    "\n",
    "# source folder\n",
    "source_path = 'data/synthetic_data/black/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xausa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xausa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# initialize ResNet for pre-processing for each image in order to extract more relevant feature and not work directly with pixels\n",
    "model = init_resnet(seed_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get production data\n",
    "normal_df = df_from_folder(normal_b_path, model)\n",
    "drifted_df = df_from_folder(drifted_b_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get source data as training data for reference\n",
    "\n",
    "len_dataframe = min(len(normal_df), len(drifted_df))    # needed for get balanced length of source data (training data >>> production data)\n",
    "\n",
    "# Select a part of development (source) data as training data and a part as reference for the comparison with the production data\n",
    "imagesList = os.listdir(source_path)        # list of all source data\n",
    "rs = np.random.RandomState(seed_split)\n",
    "rs.shuffle(imagesList)\n",
    "\n",
    "# get lists of images for train set and source set (development set on which compare production data to)\n",
    "source_list = imagesList[0:len_dataframe]          # images we will use to make comparison\n",
    "train_list = imagesList[len_dataframe+1 : 2*len_dataframe]      # images for training\n",
    "\n",
    "# get source df and training dataframe\n",
    "source_df = df_from_folder(source_path, model,  source_list)\n",
    "train_df = df_from_folder(source_path, model, train_list) # we need also train_df for training autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize scaler and dimensionality reductors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize standar scaler\n",
    "standard_scaler = init_scaler(train_df)\n",
    "\n",
    "# scaling dataframes for autoencoders\n",
    "train_scaled = scale_dataset(train_df, standard_scaler)\n",
    "source_scaled = scale_dataset(source_df, standard_scaler)\n",
    "normal_scaled = scale_dataset(normal_df, standard_scaler)\n",
    "drifted_scaled = scale_dataset(drifted_df, standard_scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xausa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\umap\\umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\xausa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\xausa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:From c:\\Users\\xausa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\xausa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3/3 [==============================] - 2s 169ms/step - loss: 0.3881 - mae: 0.3881 - val_loss: 0.3795 - val_mae: 0.3795\n",
      "Epoch 2/15\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3586 - mae: 0.3586 - val_loss: 0.3387 - val_mae: 0.3387\n",
      "Epoch 3/15\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.3157 - mae: 0.3157 - val_loss: 0.2950 - val_mae: 0.2950\n",
      "Epoch 4/15\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.2723 - mae: 0.2723 - val_loss: 0.2582 - val_mae: 0.2582\n",
      "Epoch 5/15\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.2358 - mae: 0.2358 - val_loss: 0.2287 - val_mae: 0.2287\n",
      "Epoch 6/15\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.2069 - mae: 0.2069 - val_loss: 0.2080 - val_mae: 0.2080\n",
      "Epoch 7/15\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.1878 - mae: 0.1878 - val_loss: 0.1965 - val_mae: 0.1965\n",
      "Epoch 8/15\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.1771 - mae: 0.1771 - val_loss: 0.1890 - val_mae: 0.1890\n",
      "Epoch 9/15\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.1699 - mae: 0.1699 - val_loss: 0.1844 - val_mae: 0.1844\n",
      "Epoch 10/15\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.1653 - mae: 0.1653 - val_loss: 0.1807 - val_mae: 0.1807\n",
      "Epoch 11/15\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.1621 - mae: 0.1621 - val_loss: 0.1785 - val_mae: 0.1785\n",
      "Epoch 12/15\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.1597 - mae: 0.1597 - val_loss: 0.1767 - val_mae: 0.1767\n",
      "Epoch 13/15\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.1581 - mae: 0.1581 - val_loss: 0.1752 - val_mae: 0.1752\n",
      "Epoch 14/15\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1566 - mae: 0.1566 - val_loss: 0.1745 - val_mae: 0.1745\n",
      "Epoch 15/15\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.1553 - mae: 0.1553 - val_loss: 0.1730 - val_mae: 0.1730\n"
     ]
    }
   ],
   "source": [
    "# initialize dimensionality reduction\n",
    "reducer_pca, reducer_umap, U_encoder_layer, T_encoder_layer = initialize_DimReduction(seed_metrics, source_df,  train_scaled, source_scaled, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 0s/step\n",
      "3/3 [==============================] - 0s 7ms/step\n",
      "3/3 [==============================] - 0s 8ms/step\n",
      "3/3 [==============================] - 0s 9ms/step\n",
      "3/3 [==============================] - 0s 0s/step\n",
      "3/3 [==============================] - 0s 0s/step\n"
     ]
    }
   ],
   "source": [
    "# Apply dimensionality reduction \n",
    "# reduce source dataframe\n",
    "source_dim_red, _ = reduced_on_drift_kdim(source_df,  info_dataset,  reducer_pca, reducer_umap, U_encoder_layer, T_encoder_layer)\n",
    "# reduce production data 'normal' day\n",
    "normal_dim_red, info_drift_normal = reduced_on_drift_kdim(normal_df,  info_dataset,  reducer_pca, reducer_umap, U_encoder_layer, T_encoder_layer,  sigma=42, drift = 'normal')\n",
    "# reduce production data 'drifted' day\n",
    "drifted_dim_red, info_drift_drifted = reduced_on_drift_kdim(drifted_df,  info_dataset,  reducer_pca, reducer_umap, U_encoder_layer, T_encoder_layer,  sigma=42, drift = 'production_drift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tests on data from 'normal' day\n",
    "test_on_reduced_kdim(normal_df, source_df, normal_dim_red, source_dim_red, seed_metrics, resultFile, info_drift_normal, k)\n",
    "# apply tests on data from 'drifted' day\n",
    "test_on_reduced_kdim(drifted_df, source_df, drifted_dim_red, source_dim_red, seed_metrics, resultFile, info_drift_drifted, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
