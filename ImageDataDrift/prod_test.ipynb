{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to apply metrics on production data. We are considering two datasets: one in a 'normal' day and the second one in a 'drifted' day, i.e. a day when drifted occoured in producction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('Utils'))\n",
    "sys.path.append(os.path.abspath('data'))\n",
    "sys.path.append(os.path.abspath('thresholds_and_results'))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils_driftSimulating import  create_black_folder\n",
    "from utils_resNet import init_resnet, df_from_folder\n",
    "from utils_dimRedDef import init_scaler, scale_dataset, initialize_DimReduction\n",
    "from utils_generateTests import reduced_on_drift_kdim, test_on_reduced_kdim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "seed_split = 1\n",
    "seed_drift = 10\n",
    "seed_metrics = 100\n",
    "info_dataset = [seed_split, seed_drift, seed_metrics]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of dimensions for dimensionality reduction\n",
    "k = 6\n",
    "# csv file where saving results\n",
    "resultFile = 'thresholds_and_results/6dim/prodResults_6d.csv'\n",
    "\n",
    "resultFile = 'thresholds_and_results/6dim/prod6d_prova.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories for original production data\n",
    "normal_path = 'data/original_data/normal/'\n",
    "drifted_path = 'data/original_data/drifted/'\n",
    "\n",
    "# directories where saving production black images\n",
    "normal_b_path = 'data/synthetic_data/normal_black/'\n",
    "drifted_b_path = 'data/synthetic_data/drifted_black/'\n",
    "\n",
    "# # apply black filter, to do just once because we apply black filter to production images\n",
    "# create_black_folder(normal_path, normal_b_path)\n",
    "# create_black_folder(drifted_path, drifted_b_path)\n",
    "\n",
    "# source folder\n",
    "source_path = 'data/synthetic_data/black/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize ResNet for pre-processing for each image in order to extract more relevant feature and not work directly with pixels\n",
    "model = init_resnet(seed_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get production data\n",
    "normal_df = df_from_folder(normal_b_path, model)\n",
    "drifted_df = df_from_folder(drifted_b_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get source data as training data for reference\n",
    "\n",
    "len_dataframe = min(len(normal_df), len(drifted_df))    # needed for get balanced length of source data (training data >>> production data)\n",
    "\n",
    "# Select a part of development (source) data as training data and a part as reference for the comparison with the production data\n",
    "imagesList = os.listdir(source_path)        # list of all source data\n",
    "rs = np.random.RandomState(seed_split)\n",
    "rs.shuffle(imagesList)\n",
    "\n",
    "# get lists of images for train set and source set (development set on which compare production data to)\n",
    "source_list = imagesList[0:len_dataframe]          # images we will use to make comparison\n",
    "train_list = imagesList[len_dataframe+1 : 2*len_dataframe]      # images for training\n",
    "\n",
    "# get source df and training dataframe\n",
    "source_df = df_from_folder(source_path, model,  source_list)\n",
    "train_df = df_from_folder(source_path, model, train_list) # we need also train_df for training autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize scaler and dimensionality reductors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize standar scaler\n",
    "standard_scaler = init_scaler(train_df)\n",
    "\n",
    "# scaling dataframes for autoencoders\n",
    "train_scaled = scale_dataset(train_df, standard_scaler)\n",
    "source_scaled = scale_dataset(source_df, standard_scaler)\n",
    "normal_scaled = scale_dataset(normal_df, standard_scaler)\n",
    "drifted_scaled = scale_dataset(drifted_df, standard_scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dimensionality reduction\n",
    "reducer_pca, reducer_umap, U_encoder_layer, T_encoder_layer = initialize_DimReduction(seed_metrics, source_df,  train_scaled, source_scaled, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction \n",
    "# reduce source dataframe\n",
    "source_dim_red, _ = reduced_on_drift_kdim(source_df,  info_dataset,  reducer_pca, reducer_umap, U_encoder_layer, T_encoder_layer)\n",
    "# reduce production data 'normal' day\n",
    "normal_dim_red, info_drift_normal = reduced_on_drift_kdim(normal_df,  info_dataset,  reducer_pca, reducer_umap, U_encoder_layer, T_encoder_layer,  sigma=42, drift = 'normal')\n",
    "# reduce production data 'drifted' day\n",
    "drifted_dim_red, info_drift_drifted = reduced_on_drift_kdim(drifted_df,  info_dataset,  reducer_pca, reducer_umap, U_encoder_layer, T_encoder_layer,  sigma=42, drift = 'production_drift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tests on data from 'normal' day\n",
    "test_on_reduced_kdim(normal_df, source_df, normal_dim_red, source_dim_red, seed_metrics, resultFile, info_drift_normal, k)\n",
    "# apply tests on data from 'drifted' day\n",
    "test_on_reduced_kdim(drifted_df, source_df, drifted_dim_red, source_dim_red, seed_metrics, resultFile, info_drift_drifted, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
